<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebXR with Audio and Voice Recording</title>
    <style>
        body { margin: 0; overflow: hidden; }
        canvas { display: block; }
    </style>
</head>
<body>
<script type="module">
import * as THREE from 'https://threejs.org/build/three.module.js';
import { ARButton } from 'https://threejs.org/examples/jsm/webxr/ARButton.js';

let scene, camera, renderer;
let fadeValue = 0;
let fadingIn = false;
let audioContext, audioElement, audioSource;
let mediaRecorder;
let chunks = [];
let playButton;
let raycaster, pointer;

// Initialize WebXR, audio playback, and microphone setup
init();
animate();

async function init() {
    // Renderer setup
    renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.xr.enabled = true;
    renderer.xr.setReferenceSpaceType('local');
    document.body.appendChild(renderer.domElement);

    // AR Button for entering AR mode
    document.body.appendChild(ARButton.createButton(renderer, { requiredFeatures: ['hit-test'] }));

    // Camera and Scene setup
    scene = new THREE.Scene();
    camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, 0.01, 20);

    // Set the background color to sky blue
    scene.background = new THREE.Color(0x87CEEB);

    // Initialize audio playback
    await setupAudioPlayback();

    // Initialize microphone recording with voice activity detection
    setupMicrophoneRecording();

    // Initialize the play button in the scene
    createPlayButton();

    // Initialize raycaster for button interaction
    raycaster = new THREE.Raycaster();
    pointer = new THREE.Vector2();

    // Add event listener for mouse clicks
    window.addEventListener('click', onPointerClick);

    // Fade effect initialization
    renderer.setClearAlpha(fadeValue);
}

// Setup audio playback for a 5-minute audio file
async function setupAudioPlayback() {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    audioElement = new Audio('media/day2-all.mp3'); // Replace with actual path
    audioElement.loop = false; // Play once for 5 minutes
    audioSource = audioContext.createMediaElementSource(audioElement);
    audioSource.connect(audioContext.destination);

    // Start playback when XR session starts
    audioElement.play();
}

// Setup microphone recording with voice activity detection
function setupMicrophoneRecording() {
    // Request access to the microphone
    navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
            mediaRecorder = new MediaRecorder(stream);

            // Handle audio data chunks
            mediaRecorder.ondataavailable = event => chunks.push(event.data);

            // Start recording when voice activity is detected
            const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.continuous = true;
            recognition.interimResults = false;

            recognition.onstart = () => console.log("Voice activity detection started.");
            recognition.onresult = event => {
                if (mediaRecorder.state === "inactive") {
                    mediaRecorder.start();
                    console.log("Recording started due to voice activity.");
                }
            };

            recognition.onend = () => {
                if (mediaRecorder.state === "recording") {
                    mediaRecorder.stop();
                    console.log("Recording stopped due to no voice activity.");
                }
                saveRecording();
            };

            recognition.start();
        })
        .catch(error => console.error("Microphone access denied:", error));
}

function saveRecording() {
    const blob = new Blob(chunks, { type: 'audio/webm' });
    const url = URL.createObjectURL(blob);
    const link = document.createElement('a');
    link.href = url;
    link.download = 'recorded-voice.webm';
    link.click();
    chunks = []; // Clear the chunks for the next recording session
}

// Create a 3D button to start audio playback
function createPlayButton() {
    const buttonGeometry = new THREE.PlaneGeometry(0.2, 0.1);
    const buttonMaterial = new THREE.MeshBasicMaterial({ color: 0x00ff00, side: THREE.DoubleSide });
    playButton = new THREE.Mesh(buttonGeometry, buttonMaterial);
    playButton.position.set(0, 1, -1); // Position it in front of the user
    scene.add(playButton);

    // Add button label
    const canvas = document.createElement('canvas');
    const context = canvas.getContext('2d');
    context.font = '20px Arial';
    context.fillStyle = 'white';
    context.fillText('Play Audio', 10, 20);
    const texture = new THREE.CanvasTexture(canvas);
    const labelMaterial = new THREE.MeshBasicMaterial({ map: texture });
    const label = new THREE.Mesh(new THREE.PlaneGeometry(0.2, 0.1), labelMaterial);
    label.position.set(0, 1, -0.99);
    scene.add(label);
}

// Handle clicks on the play button
function onPointerClick(event) {
    pointer.x = (event.clientX / window.innerWidth) * 2 - 1;
    pointer.y = -(event.clientY / window.innerHeight) * 2 + 1;

    raycaster.setFromCamera(pointer, camera);
    const intersects = raycaster.intersectObjects([playButton]);

    if (intersects.length > 0) {
        // Start audio playback
        audioElement.play();
        console.log("Audio playback started.");
        fadingIn = true;
        console.log("Starting fade in...");
    }
}

// Animation loop with fade effect
function animate() {
    renderer.setAnimationLoop(render);
}

function render() {
    // Fade effect
    if (fadingIn) {
        fadeValue = Math.min(fadeValue + 0.01, 1);
        if (fadeValue >= 1) {
            fadingIn = false;
        }
    }
    renderer.setClearAlpha(fadeValue);

    // Render scene
    renderer.render(scene, camera);
}

</script>
</body>
</html>