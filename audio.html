<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>WebXR Speech Waveform</title>
  <script src="https://cdn.jsdelivr.net/npm/three@latest/build/three.min.js"></script>
  <!-- <script src="https://cdn.jsdelivr.net/npm/webrtcvad"></script> -->
  <!-- <script src="https://cdn.jsdelivr.net/npm/webrtcvad@1.0.1/dist/index.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/@ennuicastr/webrtcvad.js@0.1.1/webrtcvad.min.js"></script>
</head>
<body style="margin: 0; overflow: hidden;">
  <script type="module">
    import { ARButton } from "https://cdn.jsdelivr.net/npm/three@0.155.0/examples/jsm/webxr/ARButton.js";
    // import { VAD } from "https://cdn.jsdelivr.net/npm/webrtcvad@1.0.1/dist/index.js";
    // import { WebRtcVad } from "https://cdn.jsdelivr.net/npm/@ennuicastr/webrtcvad.js@0.1.1/webrtcvad.min.js";

    // Create a Three.js scene
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
    camera.position.set(0, 0, 2);

    const renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.xr.enabled = true;
    document.body.appendChild(renderer.domElement);

    // AR Button
    document.body.appendChild(ARButton.createButton(renderer));

    let audioContext;
    let analyser;
    let dataArray;
    let audioStarted = false;

    let waveformGeometry, waveformMaterial, waveformLine, waveformVertices;
    let bufferLength = 256;

    let myvad; // WebRTC VAD
    let isSpeaking = false;

    async function initAudio() {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const source = audioContext.createMediaStreamSource(stream);
      analyser = audioContext.createAnalyser();
      analyser.fftSize = bufferLength;

      source.connect(analyser);

      dataArray = new Uint8Array(bufferLength);

      // Set the initial waveform geometry
      waveformVertices = new Float32Array(bufferLength * 3);
      for (let i = 0; i < bufferLength; i++) {
        waveformVertices[i * 3] = i / bufferLength - 0.5; // x
        waveformVertices[i * 3 + 1] = 0;                 // y
        waveformVertices[i * 3 + 2] = -0.75;            // z
      }
      waveformGeometry.setAttribute('position', new THREE.BufferAttribute(waveformVertices, 3));

      // Initialize WebRTC VAD
      myvad = new WebRtcVad({ mode: 3 }); // Aggressive mode

      const audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);
      source.connect(audioProcessor);
      audioProcessor.connect(audioContext.destination);

      audioProcessor.onaudioprocess = (e) => {
        const inputBuffer = e.inputBuffer.getChannelData(0);
        const frame = new Int16Array(inputBuffer.length);
        for (let i = 0; i < inputBuffer.length; i++) {
          frame[i] = Math.max(-32768, Math.min(32767, inputBuffer[i] * 32768)); // Convert to 16-bit PCM
        }
        isSpeaking = myvad.process(frame, audioContext.sampleRate) === 1; // 1 indicates speech detected
        waveformLine.visible = isSpeaking; // Toggle waveform visibility
      };
    }

    function addWaveform() {
      // Create a line geometry to display the waveform
      waveformGeometry = new THREE.BufferGeometry();
      waveformMaterial = new THREE.LineBasicMaterial({ color: 0x00ff00 });
      waveformLine = new THREE.Line(waveformGeometry, waveformMaterial);
      waveformLine.visible = false; // Initially hide the waveform
      scene.add(waveformLine);
    }

    function updateWaveform() {
      if (analyser && isSpeaking) {
        analyser.getByteTimeDomainData(dataArray);

        const positions = waveformGeometry.attributes.position.array;
        for (let i = 0; i < dataArray.length; i++) {
          positions[i * 3 + 1] = dataArray[i] / 128.0 - 1.0; // Map data to -1 to 1
        }
        waveformGeometry.attributes.position.needsUpdate = true;
      }
    }

    // Animation loop
    function animate() {
      renderer.setAnimationLoop(() => {
        if (audioStarted) updateWaveform();
        renderer.render(scene, camera);
      });
    }

    // Handle XR session start
    renderer.xr.addEventListener('sessionstart', () => {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      initAudio();
      addWaveform();
      audioStarted = true;
    });

    animate();
  </script>
</body>
</html>
